{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id='1'></a>\n## _Import Libraries and Load Data_","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#general packages for data manipulation\nimport os\nimport pandas as pd\nimport numpy as np\n#visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n#consistent sized plot \n#handle the warnings in the code\nimport warnings\nwarnings.filterwarnings(action='ignore',category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore',category=FutureWarning)\n#text preprocessing libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nimport re\n\n#display pandas dataframe columns \npd.options.display.max_columns = None\n\n#import the models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:37.530567Z","iopub.execute_input":"2022-11-16T06:18:37.53132Z","iopub.status.idle":"2022-11-16T06:18:37.545725Z","shell.execute_reply.started":"2022-11-16T06:18:37.531273Z","shell.execute_reply":"2022-11-16T06:18:37.544902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the csv file as a pandas dataframe\n#ISO-8859-1\ntweet = pd.read_csv('/kaggle/input/twitter-hate-speech/TwitterHate.csv',delimiter=',',engine='python',encoding='utf-8-sig')\ntweet.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:37.558873Z","iopub.execute_input":"2022-11-16T06:18:37.559445Z","iopub.status.idle":"2022-11-16T06:18:37.730397Z","shell.execute_reply.started":"2022-11-16T06:18:37.559397Z","shell.execute_reply":"2022-11-16T06:18:37.729081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(tweet, title=\"Profiling Report\")\nprofile","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:37.731621Z","iopub.execute_input":"2022-11-16T06:18:37.731854Z","iopub.status.idle":"2022-11-16T06:18:43.665902Z","shell.execute_reply.started":"2022-11-16T06:18:37.731829Z","shell.execute_reply":"2022-11-16T06:18:43.664853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get rid of the identifier number of the tweet\n# Drop ID feature from data\ntweet.drop('id',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:43.66767Z","iopub.execute_input":"2022-11-16T06:18:43.668099Z","iopub.status.idle":"2022-11-16T06:18:43.675125Z","shell.execute_reply.started":"2022-11-16T06:18:43.668058Z","shell.execute_reply":"2022-11-16T06:18:43.673957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot('label',data=tweet)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:43.676671Z","iopub.execute_input":"2022-11-16T06:18:43.677131Z","iopub.status.idle":"2022-11-16T06:18:43.807717Z","shell.execute_reply.started":"2022-11-16T06:18:43.677092Z","shell.execute_reply":"2022-11-16T06:18:43.806543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a copy of the original data to work with \ndf = tweet.copy()","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:43.809002Z","iopub.execute_input":"2022-11-16T06:18:43.809258Z","iopub.status.idle":"2022-11-16T06:18:43.814204Z","shell.execute_reply.started":"2022-11-16T06:18:43.809232Z","shell.execute_reply":"2022-11-16T06:18:43.813145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n## _Text Cleaning_\n## _Data Preprocessing_","metadata":{}},{"cell_type":"markdown","source":"<a name='2-1'></a>\n### _Handle Diacritics using text normalization_","metadata":{}},{"cell_type":"code","source":"# Create a simplify function to handle diacrtics (ex:, ', ~, \" etc.) from the data\ndef simplify(text):\n    '''Function to handle the diacritics in the text'''\n    import unicodedata\n    try:\n        text = unicode(text, 'utf-8')\n    except NameError:\n        pass\n    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n    return str(text)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:43.815635Z","iopub.execute_input":"2022-11-16T06:18:43.816174Z","iopub.status.idle":"2022-11-16T06:18:43.831439Z","shell.execute_reply.started":"2022-11-16T06:18:43.816145Z","shell.execute_reply":"2022-11-16T06:18:43.829541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply simplify function on the data\ndf['tweet'] = df['tweet'].apply(simplify)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:43.833932Z","iopub.execute_input":"2022-11-16T06:18:43.834367Z","iopub.status.idle":"2022-11-16T06:18:43.904138Z","shell.execute_reply.started":"2022-11-16T06:18:43.834319Z","shell.execute_reply":"2022-11-16T06:18:43.90314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2-2'></a>\n### _Remove user handles_","metadata":{}},{"cell_type":"code","source":"#remove all the user handles --> strings starting with @ (ex: @user1, @user2, etc.)\ndf['tweet'].replace(r'@\\w+','',regex=True,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:43.9074Z","iopub.execute_input":"2022-11-16T06:18:43.907757Z","iopub.status.idle":"2022-11-16T06:18:43.96967Z","shell.execute_reply.started":"2022-11-16T06:18:43.907724Z","shell.execute_reply":"2022-11-16T06:18:43.968171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2-3'></a>\n### _Remove the urls_","metadata":{}},{"cell_type":"code","source":"# Remove urls from the data set\ndf['tweet'].replace(r'http\\S+','',regex=True,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:43.97178Z","iopub.execute_input":"2022-11-16T06:18:43.972066Z","iopub.status.idle":"2022-11-16T06:18:44.026596Z","shell.execute_reply.started":"2022-11-16T06:18:43.972032Z","shell.execute_reply":"2022-11-16T06:18:44.025627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2-4'></a>\n### _Tokenize using tweet tokenizer_","metadata":{}},{"cell_type":"code","source":"#tokenize the tweets in the dataframe using TweetTokenizer\n# tokenizer breaks down sentences into each word (breaks them by leveraging spaces in sentences)\ntokenizer = TweetTokenizer(preserve_case=True)\ndf['tweet'] = df['tweet'].apply(tokenizer.tokenize)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:44.027944Z","iopub.execute_input":"2022-11-16T06:18:44.028437Z","iopub.status.idle":"2022-11-16T06:18:45.288389Z","shell.execute_reply.started":"2022-11-16T06:18:44.02839Z","shell.execute_reply":"2022-11-16T06:18:45.287112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Review the tokenized tweets\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:45.28998Z","iopub.execute_input":"2022-11-16T06:18:45.290332Z","iopub.status.idle":"2022-11-16T06:18:45.302932Z","shell.execute_reply.started":"2022-11-16T06:18:45.290301Z","shell.execute_reply":"2022-11-16T06:18:45.301621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2-5'></a>\n### _Remove Stopwords_\n_Append more words to be removed from the text - example rt and amp which occur very frequently_","metadata":{}},{"cell_type":"code","source":"# General stop words are a, an, the, he, she, is, in, be, by etc.)\nstop_words = stopwords.words('english')\n\n#add additional stop words to be removed from the text\nadd_list = ['amp','rt','u',\"can't\",'ur']\n\nfor words in add_list:\n    stop_words.append(words)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:45.304477Z","iopub.execute_input":"2022-11-16T06:18:45.30482Z","iopub.status.idle":"2022-11-16T06:18:45.313466Z","shell.execute_reply.started":"2022-11-16T06:18:45.304781Z","shell.execute_reply":"2022-11-16T06:18:45.311772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the last 10 stop words among all stop words\nstop_words[-10:]","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:45.315163Z","iopub.execute_input":"2022-11-16T06:18:45.315536Z","iopub.status.idle":"2022-11-16T06:18:45.331346Z","shell.execute_reply.started":"2022-11-16T06:18:45.315482Z","shell.execute_reply":"2022-11-16T06:18:45.329711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define function to remove stop words from data set\ndef remove_stopwords(text):\n    '''Function to remove the stop words from the text corpus'''\n    clean_text = [word for word in text if not word in stop_words]\n    return clean_text    ","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:45.333109Z","iopub.execute_input":"2022-11-16T06:18:45.333443Z","iopub.status.idle":"2022-11-16T06:18:45.33909Z","shell.execute_reply.started":"2022-11-16T06:18:45.333407Z","shell.execute_reply":"2022-11-16T06:18:45.337983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove the stop words from the tweets\ndf['tweet'] = df['tweet'].apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:45.340252Z","iopub.execute_input":"2022-11-16T06:18:45.340675Z","iopub.status.idle":"2022-11-16T06:18:46.44674Z","shell.execute_reply.started":"2022-11-16T06:18:45.340641Z","shell.execute_reply":"2022-11-16T06:18:46.445213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tweet'].head()","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:46.448071Z","iopub.execute_input":"2022-11-16T06:18:46.44834Z","iopub.status.idle":"2022-11-16T06:18:46.455412Z","shell.execute_reply.started":"2022-11-16T06:18:46.448309Z","shell.execute_reply":"2022-11-16T06:18:46.454702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2-6'></a>\n### _Spelling corrections_","metadata":{}},{"cell_type":"code","source":"#apply spelling correction on a sample text\nfrom textblob import TextBlob\nsample = 'amazng man you did it finallyy'\ntxtblob = TextBlob(sample)\ncorrected_text = txtblob.correct()\nprint(corrected_text)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:46.456375Z","iopub.execute_input":"2022-11-16T06:18:46.456623Z","iopub.status.idle":"2022-11-16T06:18:46.47236Z","shell.execute_reply.started":"2022-11-16T06:18:46.456599Z","shell.execute_reply":"2022-11-16T06:18:46.470488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply auto correct on the twitter data set\n#textblob expect a string to be passed and not a list of strings\nfrom textblob import TextBlob\n\ndef spell_check(text):\n    '''Function to do spelling correction using '''\n    txtblob = TextBlob(text)\n    corrected_text = txtblob.correct()\n    return corrected_text\n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:46.474472Z","iopub.execute_input":"2022-11-16T06:18:46.474878Z","iopub.status.idle":"2022-11-16T06:18:46.484049Z","shell.execute_reply.started":"2022-11-16T06:18:46.47484Z","shell.execute_reply":"2022-11-16T06:18:46.483174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2-7'></a>\n### _Remove # symbols while retaining the text_","metadata":{}},{"cell_type":"code","source":"#Defining function to remove hash symbols from data set\ndef remove_hashsymbols(text):\n    '''Function to remove the hashtag symbol from the text'''\n    pattern = re.compile(r'#')\n    text = ' '.join(text)\n    clean_text = re.sub(pattern,'',text)\n    return tokenizer.tokenize(clean_text)    ","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:46.485178Z","iopub.execute_input":"2022-11-16T06:18:46.485552Z","iopub.status.idle":"2022-11-16T06:18:46.499542Z","shell.execute_reply.started":"2022-11-16T06:18:46.485493Z","shell.execute_reply":"2022-11-16T06:18:46.498716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tweet'] = df['tweet'].apply(remove_hashsymbols)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:46.500637Z","iopub.execute_input":"2022-11-16T06:18:46.500912Z","iopub.status.idle":"2022-11-16T06:18:47.357546Z","shell.execute_reply.started":"2022-11-16T06:18:46.500874Z","shell.execute_reply":"2022-11-16T06:18:47.356869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2-8'></a>\n### _Remove single and double length characters_","metadata":{}},{"cell_type":"code","source":"# Define function to remove words containing only 1 or 2 chacters like 'a', 'ig' etc.\ndef rem_shortwords(text):\n    '''Function to remove the short words of length 1 and 2 characters'''\n    '''Arguments: \n       text: string\n       returns: string without containing words of length 1 and 2'''\n    lengths = [1,2]\n    new_text = ' '.join(text)\n    for word in text:\n        text = [word for word in tokenizer.tokenize(new_text) if not len(word) in lengths]\n        \n    return new_text       \n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:47.358495Z","iopub.execute_input":"2022-11-16T06:18:47.358822Z","iopub.status.idle":"2022-11-16T06:18:47.36331Z","shell.execute_reply.started":"2022-11-16T06:18:47.358798Z","shell.execute_reply":"2022-11-16T06:18:47.362589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the short words\ndf['tweet'] = df['tweet'].apply(rem_shortwords)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:47.364659Z","iopub.execute_input":"2022-11-16T06:18:47.364969Z","iopub.status.idle":"2022-11-16T06:18:55.114128Z","shell.execute_reply.started":"2022-11-16T06:18:47.364938Z","shell.execute_reply":"2022-11-16T06:18:55.113031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Again apply tokenizer to break the sentences into words\ndf['tweet'] = df['tweet'].apply(tokenizer.tokenize)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:55.117848Z","iopub.execute_input":"2022-11-16T06:18:55.118223Z","iopub.status.idle":"2022-11-16T06:18:55.826267Z","shell.execute_reply.started":"2022-11-16T06:18:55.118185Z","shell.execute_reply":"2022-11-16T06:18:55.825659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2-9'></a>\n### _Remove digits_","metadata":{}},{"cell_type":"code","source":"# Defining a function to remove digits from the dataset\ndef rem_digits(text):\n    '''Function to remove the digits from the list of strings'''\n    no_digits = []\n    for word in text:\n        no_digits.append(re.sub(r'\\d','',word))\n    return ' '.join(no_digits)   ","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:55.827568Z","iopub.execute_input":"2022-11-16T06:18:55.827867Z","iopub.status.idle":"2022-11-16T06:18:55.831273Z","shell.execute_reply.started":"2022-11-16T06:18:55.827843Z","shell.execute_reply":"2022-11-16T06:18:55.830582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tweet'] = df['tweet'].apply(rem_digits)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:55.832184Z","iopub.execute_input":"2022-11-16T06:18:55.83243Z","iopub.status.idle":"2022-11-16T06:18:56.242758Z","shell.execute_reply.started":"2022-11-16T06:18:55.832406Z","shell.execute_reply":"2022-11-16T06:18:56.241794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tweet'] = df['tweet'].apply(tokenizer.tokenize)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:56.243902Z","iopub.execute_input":"2022-11-16T06:18:56.244165Z","iopub.status.idle":"2022-11-16T06:18:57.180024Z","shell.execute_reply.started":"2022-11-16T06:18:56.244134Z","shell.execute_reply":"2022-11-16T06:18:57.178844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2-10'></a>\n### _Remove special characters_\n","metadata":{}},{"cell_type":"code","source":"# Defining a function to remove special charcters from the dataset\ndef rem_nonalpha(text):\n    '''Function to remove the non-alphanumeric characters from the text'''\n    text = [word for word in text if word.isalpha()]\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.181436Z","iopub.execute_input":"2022-11-16T06:18:57.181791Z","iopub.status.idle":"2022-11-16T06:18:57.186938Z","shell.execute_reply.started":"2022-11-16T06:18:57.181753Z","shell.execute_reply":"2022-11-16T06:18:57.185804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove the non alpha numeric characters from the tweet tokens\ndf['tweet'] = df['tweet'].apply(rem_nonalpha)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.188357Z","iopub.execute_input":"2022-11-16T06:18:57.188649Z","iopub.status.idle":"2022-11-16T06:18:57.295042Z","shell.execute_reply.started":"2022-11-16T06:18:57.188616Z","shell.execute_reply":"2022-11-16T06:18:57.294079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Visualization for hate and non hate tweets\nsns.countplot(df['label'])\nplt.title('Offensive vs Nonoffensive Tweets')\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.296359Z","iopub.execute_input":"2022-11-16T06:18:57.296725Z","iopub.status.idle":"2022-11-16T06:18:57.412762Z","shell.execute_reply.started":"2022-11-16T06:18:57.296684Z","shell.execute_reply":"2022-11-16T06:18:57.411589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3-2'></a>\n### _Check out the top terms in the tweets_","metadata":{"execution":{"iopub.status.busy":"2021-08-13T08:28:31.023999Z","iopub.execute_input":"2021-08-13T08:28:31.024391Z","iopub.status.idle":"2021-08-13T08:28:31.13622Z","shell.execute_reply.started":"2021-08-13T08:28:31.024358Z","shell.execute_reply":"2021-08-13T08:28:31.135427Z"}}},{"cell_type":"code","source":"# Data visualization on the most used terms in tweets\nfrom collections import Counter\nresults = Counter()\ndf['tweet'].apply(results.update)\n#print the top 10 most common terms in the tweet \nprint(results.most_common(15))","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.414009Z","iopub.execute_input":"2022-11-16T06:18:57.414283Z","iopub.status.idle":"2022-11-16T06:18:57.553734Z","shell.execute_reply.started":"2022-11-16T06:18:57.414255Z","shell.execute_reply":"2022-11-16T06:18:57.553081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the frequency of the 15 mostly used terms \nfrequency = nltk.FreqDist(results)\nplt.title('Top 15 Mostly used Terms')\nfrequency.plot(15,cumulative=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.55466Z","iopub.execute_input":"2022-11-16T06:18:57.554977Z","iopub.status.idle":"2022-11-16T06:18:57.743907Z","shell.execute_reply.started":"2022-11-16T06:18:57.554952Z","shell.execute_reply":"2022-11-16T06:18:57.74245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Love is the most frequently used word followed by day, happy etc. This is expected as there are more non hate tweets than hate tweets in the dataset_","metadata":{}},{"cell_type":"markdown","source":"<a id='4'></a>\n## _Predictive Modeling_","metadata":{}},{"cell_type":"markdown","source":"### _Data Formatting for Predictive Modeling_","metadata":{}},{"cell_type":"code","source":"# A final check on the dataset before applying the ML models\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.745081Z","iopub.execute_input":"2022-11-16T06:18:57.745425Z","iopub.status.idle":"2022-11-16T06:18:57.759392Z","shell.execute_reply.started":"2022-11-16T06:18:57.745395Z","shell.execute_reply":"2022-11-16T06:18:57.75803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Add tokens to make the preprocessed tweets\ndf['tweet'] = df['tweet'].apply(lambda x: ' '.join(x))","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.760851Z","iopub.execute_input":"2022-11-16T06:18:57.761183Z","iopub.status.idle":"2022-11-16T06:18:57.806667Z","shell.execute_reply.started":"2022-11-16T06:18:57.761149Z","shell.execute_reply":"2022-11-16T06:18:57.805159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split the data into input X and output y\nX = df['tweet']\ny = df['label']","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.808062Z","iopub.execute_input":"2022-11-16T06:18:57.808452Z","iopub.status.idle":"2022-11-16T06:18:57.817385Z","shell.execute_reply.started":"2022-11-16T06:18:57.808424Z","shell.execute_reply":"2022-11-16T06:18:57.816614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split the dataset into train and test data\ntest_size = 0.25 #splitting the train and test data in 75%:25% ratio \nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=23,stratify=df['label'])\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.818495Z","iopub.execute_input":"2022-11-16T06:18:57.818932Z","iopub.status.idle":"2022-11-16T06:18:57.84843Z","shell.execute_reply.started":"2022-11-16T06:18:57.818897Z","shell.execute_reply":"2022-11-16T06:18:57.847065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4-2'></a>\n### _Use tf-idf as a feature to get into the vector space model_\n","metadata":{}},{"cell_type":"code","source":"#import tfidf vectorizer\n#To convert texts to a matrix of TF-IDF features. \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.850153Z","iopub.execute_input":"2022-11-16T06:18:57.850468Z","iopub.status.idle":"2022-11-16T06:18:57.855117Z","shell.execute_reply.started":"2022-11-16T06:18:57.850433Z","shell.execute_reply":"2022-11-16T06:18:57.853606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the maximum number of features by term frequency to be considered for ML Models \nvectorizer = TfidfVectorizer(max_features=4500)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.856898Z","iopub.execute_input":"2022-11-16T06:18:57.857287Z","iopub.status.idle":"2022-11-16T06:18:57.869871Z","shell.execute_reply.started":"2022-11-16T06:18:57.857246Z","shell.execute_reply":"2022-11-16T06:18:57.869127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting the tdif vectorizer on the training dataset\nX_train = vectorizer.fit_transform(X_train)\n#transform the test data and apply the tdif vectorizer\nX_test = vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:57.8709Z","iopub.execute_input":"2022-11-16T06:18:57.871275Z","iopub.status.idle":"2022-11-16T06:18:58.26406Z","shell.execute_reply.started":"2022-11-16T06:18:57.871247Z","shell.execute_reply":"2022-11-16T06:18:58.262136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the shape\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:58.265679Z","iopub.execute_input":"2022-11-16T06:18:58.266002Z","iopub.status.idle":"2022-11-16T06:18:58.275241Z","shell.execute_reply.started":"2022-11-16T06:18:58.265961Z","shell.execute_reply":"2022-11-16T06:18:58.273419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n#define the number of folds \nseed = 51\nfolds = StratifiedKFold(n_splits=4,shuffle=True, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:58.276492Z","iopub.execute_input":"2022-11-16T06:18:58.276838Z","iopub.status.idle":"2022-11-16T06:18:58.287248Z","shell.execute_reply.started":"2022-11-16T06:18:58.27681Z","shell.execute_reply":"2022-11-16T06:18:58.285673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"model_factory = [CatBoostClassifier(verbose=False), \n                 LGBMClassifier(objective='binary'),\n                 RandomForestClassifier(n_jobs=-1),\n                 XGBClassifier(objective='binary:logistic'),\n                 DecisionTreeClassifier()] \n                \nval = []\nmodel_name = []\n\nfor model in model_factory:\n    mf = model.fit(X_train, y_train)\n    Pred = mf.predict(X_test)\n    scores=cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    print(model.__class__.__name__, \" : Train Accuracy: \", accuracy_score(y_test, Pred), \" : Validation Accuracy : \", np.mean(scores))\n    model_name.append(model.__class__.__name__)\n    val.append(np.mean(scores).item())","metadata":{}},{"cell_type":"code","source":"model_factory = [LogisticRegression(), \n                 LGBMClassifier(objective='binary'),\n                 RandomForestClassifier(n_jobs=-1),\n                 MultinomialNB(),\n                 DecisionTreeClassifier(),\n                 SVC(C=100.0),\n                 XGBClassifier(objective='binary:logistic')]  \n                \nval = []\nmodel_name = []\n\nfor model in model_factory:\n    mf = model.fit(X_train, y_train)\n    Pred = mf.predict(X_test)\n    scores=cross_val_score(model, X_train, y_train, cv=folds, scoring='accuracy')\n    print(model.__class__.__name__, \" : Train Accuracy: \", accuracy_score(y_test, Pred), \" : Validation Accuracy : \", np.mean(scores))\n    model_name.append(model.__class__.__name__)\n    val.append(np.mean(scores).item())","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:18:58.289268Z","iopub.execute_input":"2022-11-16T06:18:58.28969Z","iopub.status.idle":"2022-11-16T06:22:58.552088Z","shell.execute_reply.started":"2022-11-16T06:18:58.289648Z","shell.execute_reply":"2022-11-16T06:22:58.55029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (60,30))\nplt.bar(model_name, val, color=['black', 'red', 'green', 'blue', 'cyan', 'yellow', 'violet'], width=0.75)\nplt.title('Cross Validation Accuracy of Models',fontsize=70)\nplt.tick_params(axis='both', which='major',labelsize=40)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:22:58.553452Z","iopub.execute_input":"2022-11-16T06:22:58.553915Z","iopub.status.idle":"2022-11-16T06:22:59.084588Z","shell.execute_reply.started":"2022-11-16T06:22:58.553837Z","shell.execute_reply":"2022-11-16T06:22:59.083227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVC is giving the highest cross validation accuracy of 0.9605356157060853\n# Hence, tuning the SVC ML model\n\n# Hyperparameter Optimization using GridSearch CV\n# declare parameters for hyperparameter tuning\nparameters = [ {'C':[1, 10, 100], 'kernel':['linear']},\n               {'C':[1, 10, 100], 'kernel':['rbf'], 'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},\n               {'C':[1, 10, 100], 'kernel':['poly'], 'degree': [2,3] ,'gamma':[0.01,0.02,0.03]} \n              ]\n\ngrid_search = GridSearchCV(estimator = SVC(),  \n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = folds,\n                           verbose=0)\n\n\ngrid_search.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:22:59.0859Z","iopub.execute_input":"2022-11-16T06:22:59.086183Z","iopub.status.idle":"2022-11-16T06:58:06.853992Z","shell.execute_reply.started":"2022-11-16T06:22:59.08615Z","shell.execute_reply":"2022-11-16T06:58:06.852624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best score achieved during the GridSearchCV\nprint('GridSearch CV best score : {:.4f}\\n\\n'.format(grid_search.best_score_))\n\n\n# print parameters that give the best results\nprint('Parameters that give the best results :','\\n\\n', (grid_search.best_params_))\n\n\n# print estimator that was chosen by the GridSearch\nprint('\\n\\nEstimator that was chosen by the search :','\\n\\n', (grid_search.best_estimator_))","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:58:06.856586Z","iopub.execute_input":"2022-11-16T06:58:06.857114Z","iopub.status.idle":"2022-11-16T06:58:06.864478Z","shell.execute_reply.started":"2022-11-16T06:58:06.857069Z","shell.execute_reply":"2022-11-16T06:58:06.863706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate GridSearch CV score on test set\n\nprint('GridSearch CV score on test set: {0:0.4f}'.format(grid_search.score(X_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-11-16T06:58:06.86561Z","iopub.execute_input":"2022-11-16T06:58:06.865975Z","iopub.status.idle":"2022-11-16T06:58:10.232668Z","shell.execute_reply.started":"2022-11-16T06:58:06.865936Z","shell.execute_reply":"2022-11-16T06:58:10.231424Z"},"trusted":true},"execution_count":null,"outputs":[]}]}